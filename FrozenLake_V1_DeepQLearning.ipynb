{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-30T09:44:02.586485400Z",
     "start_time": "2023-08-30T09:43:58.943507700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import gather_nd\n",
    "from tensorflow.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Deep Learning Reinforcement Tutorial: Deep Q Network (DQN) = Combination of Deep Learning and Q-Learning Tutorial\n",
    "\n",
    "The class developed in this file implements the Deep Q Network (DQN) Reinforcement Learning Algorithm.\n",
    "The implementation is based on the OpenAI Gym Cart Pole environment and TensorFlow (Keras) machine learning library\n",
    "\n",
    "The webpage explaining the codes and the main idea of the DQN is given here:\n",
    "\n",
    "https://aleksandarhaber.com/deep-q-networks-dqn-in-python-from-scratch-by-using-openai-gym-and-tensorflow-reinforcement-learning-tutorial/\n",
    "\n",
    "\n",
    "Author: Aleksandar Haber \n",
    "Date: February 2023\n",
    "\n",
    "Tested on:\n",
    "\n",
    "tensorboard==2.11.2\n",
    "tensorboard-data-server==0.6.1\n",
    "tensorboard-plugin-wit==1.8.1\n",
    "tensorflow==2.11.0\n",
    "tensorflow-estimator==2.11.0\n",
    "tensorflow-intel==2.11.0\n",
    "tensorflow-io-gcs-filesystem==0.30.0\n",
    "\n",
    "keras==2.11.0\n",
    "\n",
    "gym==0.26.2\n",
    "\n",
    "\"\"\"\n",
    "# import the necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from collections import deque \n",
    "from tensorflow import gather_nd\n",
    "from tensorflow.keras.losses import mean_squared_error \n",
    "\n",
    "\n",
    "\n",
    "class DeepQLearning:\n",
    "    \n",
    "    ###########################################################################\n",
    "    #   START - __init__ function\n",
    "    ###########################################################################\n",
    "    # INPUTS: \n",
    "    # env - Cart Pole environment\n",
    "    # gamma - discount rate\n",
    "    # epsilon - parameter for epsilon-greedy approach\n",
    "    # numberEpisodes - total number of simulation episodes\n",
    "    \n",
    "            \n",
    "    def __init__(self,env,gamma,epsilon,numberEpisodes):\n",
    "        \n",
    "        \n",
    "        self.env=env\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.numberEpisodes=numberEpisodes\n",
    "        \n",
    "        # state dimension\n",
    "        self.stateDimension=4\n",
    "        # action dimension\n",
    "        self.actionDimension=2\n",
    "        # this is the maximum size of the replay buffer\n",
    "        self.replayBufferSize=300\n",
    "        # this is the size of the training batch that is randomly sampled from the replay buffer\n",
    "        self.batchReplayBufferSize=100\n",
    "        \n",
    "        # number of training episodes it takes to update the target network parameters\n",
    "        # that is, every updateTargetNetworkPeriod we update the target network parameters\n",
    "        self.updateTargetNetworkPeriod=100\n",
    "        \n",
    "        # this is the counter for updating the target network \n",
    "        # if this counter exceeds (updateTargetNetworkPeriod-1) we update the network \n",
    "        # parameters and reset the counter to zero, this process is repeated until the end of the training process\n",
    "        self.counterUpdateTargetNetwork=0\n",
    "        \n",
    "        # this sum is used to store the sum of rewards obtained during each training episode\n",
    "        self.sumRewardsEpisode=[]\n",
    "        \n",
    "        # replay buffer\n",
    "        self.replayBuffer=deque(maxlen=self.replayBufferSize)\n",
    "        \n",
    "        # this is the main network\n",
    "        # create network\n",
    "        self.mainNetwork=self.createNetwork()\n",
    "        \n",
    "        # this is the target network\n",
    "        # create network\n",
    "        self.targetNetwork=self.createNetwork()\n",
    "        \n",
    "        # copy the initial weights to targetNetwork\n",
    "        self.targetNetwork.set_weights(self.mainNetwork.get_weights())\n",
    "        \n",
    "        # this list is used in the cost function to select certain entries of the \n",
    "        # predicted and true sample matrices in order to form the loss\n",
    "        self.actionsAppend=[]\n",
    "    \n",
    "    ###########################################################################\n",
    "    #   END - __init__ function\n",
    "    ###########################################################################\n",
    "    \n",
    "    ###########################################################################\n",
    "    # START - function for defining the loss (cost) function\n",
    "    # INPUTS: \n",
    "    #\n",
    "    # y_true - matrix of dimension (self.batchReplayBufferSize,2) - this is the target \n",
    "    # y_pred - matrix of dimension (self.batchReplayBufferSize,2) - this is predicted by the network\n",
    "    # \n",
    "    # - this function will select certain row entries from y_true and y_pred to form the output \n",
    "    # the selection is performed on the basis of the action indices in the list  self.actionsAppend\n",
    "    # - this function is used in createNetwork(self) to create the network\n",
    "    #\n",
    "    # OUTPUT: \n",
    "    #    \n",
    "    # - loss - watch out here, this is a vector of (self.batchReplayBufferSize,1), \n",
    "    # with each entry being the squared error between the entries of y_true and y_pred\n",
    "    # later on, the tensor flow will compute the scalar out of this vector (mean squared error)\n",
    "    ###########################################################################    \n",
    "    \n",
    "    def my_loss_fn(self,y_true, y_pred):\n",
    "        \n",
    "        s1,s2=y_true.shape\n",
    "        #print(s1,s2)\n",
    "        \n",
    "        # this matrix defines indices of a set of entries that we want to \n",
    "        # extract from y_true and y_pred\n",
    "        # s2=2\n",
    "        # s1=self.batchReplayBufferSize\n",
    "        indices=np.zeros(shape=(s1,s2))\n",
    "        indices[:,0]=np.arange(s1)\n",
    "        indices[:,1]=self.actionsAppend\n",
    "        \n",
    "        # gather_nd and mean_squared_error are TensorFlow functions\n",
    "        loss = mean_squared_error(gather_nd(y_true,indices=indices.astype(int)), gather_nd(y_pred,indices=indices.astype(int)))\n",
    "        #print(loss)\n",
    "        return loss    \n",
    "    ###########################################################################\n",
    "    #   END - of function my_loss_fn\n",
    "    ###########################################################################\n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #   START - function createNetwork()\n",
    "    # this function creates the network\n",
    "    ###########################################################################\n",
    "    \n",
    "    # create a neural network\n",
    "    def createNetwork(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(128,input_dim=self.stateDimension,activation='relu'))\n",
    "        model.add(Dense(56,activation='relu'))\n",
    "        model.add(Dense(self.actionDimension,activation='linear'))\n",
    "        # compile the network with the custom loss defined in my_loss_fn\n",
    "        model.compile(optimizer = RMSprop(), loss = self.my_loss_fn, metrics = ['accuracy'])\n",
    "        return model\n",
    "    ###########################################################################\n",
    "    #   END - function createNetwork()\n",
    "    ###########################################################################\n",
    "            \n",
    "    ###########################################################################\n",
    "    #   START - function trainingEpisodes()\n",
    "    #   - this function simulates the episodes and calls the training function \n",
    "    #   - trainNetwork()\n",
    "    ###########################################################################\n",
    "\n",
    "    def trainingEpisodes(self):\n",
    "   \n",
    "        \n",
    "        # here we loop through the episodes\n",
    "        for indexEpisode in range(self.numberEpisodes):\n",
    "            \n",
    "            # list that stores rewards per episode - this is necessary for keeping track of convergence \n",
    "            rewardsEpisode=[]\n",
    "                       \n",
    "            print(\"Simulating episode {}\".format(indexEpisode))\n",
    "            \n",
    "            # reset the environment at the beginning of every episode\n",
    "            (currentState,_)=self.env.reset()\n",
    "                      \n",
    "            # here we step from one state to another\n",
    "            # this will loop until a terminal state is reached\n",
    "            terminalState=False\n",
    "            while not terminalState:\n",
    "                                      \n",
    "                # select an action on the basis of the current state, denoted by currentState\n",
    "                action = self.selectAction(currentState,indexEpisode)\n",
    "                \n",
    "                # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "                (nextState, reward, terminalState,_,_) = self.env.step(action)          \n",
    "                rewardsEpisode.append(reward)\n",
    "         \n",
    "                # add current state, action, reward, next state, and terminal flag to the replay buffer\n",
    "                self.replayBuffer.append((currentState,action,reward,nextState,terminalState))\n",
    "                \n",
    "                # train network\n",
    "                self.trainNetwork()\n",
    "                \n",
    "                # set the current state for the next step\n",
    "                currentState=nextState\n",
    "            \n",
    "            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))        \n",
    "            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n",
    "    ###########################################################################\n",
    "    #   END - function trainingEpisodes()\n",
    "    ###########################################################################\n",
    "            \n",
    "       \n",
    "    ###########################################################################\n",
    "    #    START - function for selecting an action: epsilon-greedy approach\n",
    "    ###########################################################################\n",
    "    # this function selects an action on the basis of the current state \n",
    "    # INPUTS: \n",
    "    # state - state for which to compute the action\n",
    "    # index - index of the current episode\n",
    "    def selectAction(self,state,index):\n",
    "        import numpy as np\n",
    "        \n",
    "        # first index episodes we select completely random actions to have enough exploration\n",
    "        # change this\n",
    "        if index<1:\n",
    "            return np.random.choice(self.actionDimension)   \n",
    "            \n",
    "        # Returns a random real number in the half-open interval [0.0, 1.0)\n",
    "        # this number is used for the epsilon greedy approach\n",
    "        randomNumber=np.random.random()\n",
    "        \n",
    "        # after index episodes, we slowly start to decrease the epsilon parameter\n",
    "        if index>200:\n",
    "            self.epsilon=0.999*self.epsilon\n",
    "        \n",
    "        # if this condition is satisfied, we are exploring, that is, we select random actions\n",
    "        if randomNumber < self.epsilon:\n",
    "            # returns a random action selected from: 0,1,...,actionNumber-1\n",
    "            return np.random.choice(self.actionDimension)            \n",
    "        \n",
    "        # otherwise, we are selecting greedy actions\n",
    "        else:\n",
    "            # we return the index where Qvalues[state,:] has the max value\n",
    "            # that is, since the index denotes an action, we select greedy actions\n",
    "                       \n",
    "            Qvalues=self.mainNetwork.predict(state.reshape(1,4))\n",
    "          \n",
    "            return np.random.choice(np.where(Qvalues[0,:]==np.max(Qvalues[0,:]))[0])\n",
    "            # here we need to return the minimum index since it can happen\n",
    "            # that there are several identical maximal entries, for example \n",
    "            # import numpy as np\n",
    "            # a=[0,1,1,0]\n",
    "            # np.where(a==np.max(a))\n",
    "            # this will return [1,2], but we only need a single index\n",
    "            # that is why we need to have np.random.choice(np.where(a==np.max(a))[0])\n",
    "            # note that zero has to be added here since np.where() returns a tuple\n",
    "    ###########################################################################\n",
    "    #    END - function selecting an action: epsilon-greedy approach\n",
    "    ###########################################################################\n",
    "    \n",
    "    ###########################################################################\n",
    "    #    START - function trainNetwork() - this function trains the network\n",
    "    ###########################################################################\n",
    "    \n",
    "    def trainNetwork(self):\n",
    "\n",
    "        # if the replay buffer has at least batchReplayBufferSize elements,\n",
    "        # then train the model \n",
    "        # otherwise wait until the size of the elements exceeds batchReplayBufferSize\n",
    "        if (len(self.replayBuffer)>self.batchReplayBufferSize):\n",
    "            \n",
    "\n",
    "            # sample a batch from the replay buffer\n",
    "            randomSampleBatch=random.sample(self.replayBuffer, self.batchReplayBufferSize)\n",
    "            \n",
    "            # here we form current state batch \n",
    "            # and next state batch\n",
    "            # they are used as inputs for prediction\n",
    "            currentStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))\n",
    "            nextStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))            \n",
    "            # this will enumerate the tuple entries of the randomSampleBatch\n",
    "            # index will loop through the number of tuples\n",
    "            for index,tupleS in enumerate(randomSampleBatch):\n",
    "                # first entry of the tuple is the current state\n",
    "                currentStateBatch[index,:]=tupleS[0]\n",
    "                # fourth entry of the tuple is the next state\n",
    "                nextStateBatch[index,:]=tupleS[3]\n",
    "            \n",
    "            # here, use the target network to predict Q-values \n",
    "            QnextStateTargetNetwork=self.targetNetwork.predict(nextStateBatch)\n",
    "            # here, use the main network to predict Q-values \n",
    "            QcurrentStateMainNetwork=self.mainNetwork.predict(currentStateBatch)\n",
    "            \n",
    "            # now, we form batches for training\n",
    "            # input for training\n",
    "            inputNetwork=currentStateBatch\n",
    "            # output for training\n",
    "            outputNetwork=np.zeros(shape=(self.batchReplayBufferSize,2))\n",
    "            \n",
    "            # this list will contain the actions that are selected from the batch \n",
    "            # this list is used in my_loss_fn to define the loss-function\n",
    "            self.actionsAppend=[]            \n",
    "            for index,(currentState,action,reward,nextState,terminated) in enumerate(randomSampleBatch):\n",
    "                \n",
    "                # if the next state is the terminal state\n",
    "                if terminated:\n",
    "                    y=reward                  \n",
    "                # if the next state if not the terminal state    \n",
    "                else:\n",
    "                    y=reward+self.gamma*np.max(QnextStateTargetNetwork[index])\n",
    "                \n",
    "                # this is necessary for defining the cost function\n",
    "                self.actionsAppend.append(action)\n",
    "                \n",
    "                # this actually does not matter since we do not use all the entries in the cost function\n",
    "                outputNetwork[index]=QcurrentStateMainNetwork[index]\n",
    "                # this is what matters\n",
    "                outputNetwork[index,action]=y\n",
    "            \n",
    "            # here, we train the network\n",
    "            self.mainNetwork.fit(inputNetwork,outputNetwork,batch_size = self.batchReplayBufferSize, verbose=0,epochs=100)     \n",
    "            \n",
    "            # after updateTargetNetworkPeriod training sessions, update the coefficients \n",
    "            # of the target network\n",
    "            # increase the counter for training the target network\n",
    "            self.counterUpdateTargetNetwork+=1  \n",
    "            if (self.counterUpdateTargetNetwork>(self.updateTargetNetworkPeriod-1)):\n",
    "                # copy the weights to targetNetwork\n",
    "                self.targetNetwork.set_weights(self.mainNetwork.get_weights())        \n",
    "                print(\"Target network updated!\")\n",
    "                print(\"Counter value {}\".format(self.counterUpdateTargetNetwork))\n",
    "                # reset the counter\n",
    "                self.counterUpdateTargetNetwork=0\n",
    "    ###########################################################################\n",
    "    #    END - function trainNetwork() \n",
    "    ###########################################################################     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T09:46:40.857456400Z",
     "start_time": "2023-08-30T09:46:40.844909600Z"
    }
   },
   "id": "51012fe9eec4c36e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating episode 0\n",
      "Sum of rewards 25.0\n",
      "Simulating episode 1\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\DL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1000us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1000us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1000us/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1000us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "4/4 [==============================] - 0s 999us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m LearningQDeep\u001B[38;5;241m=\u001B[39mDeepQLearning(env,gamma,epsilon,numberEpisodes)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# run the learning process\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[43mLearningQDeep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainingEpisodes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# get the obtained rewards in every episode\u001B[39;00m\n\u001B[0;32m     24\u001B[0m LearningQDeep\u001B[38;5;241m.\u001B[39msumRewardsEpisode\n",
      "Cell \u001B[1;32mIn[4], line 200\u001B[0m, in \u001B[0;36mDeepQLearning.trainingEpisodes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplayBuffer\u001B[38;5;241m.\u001B[39mappend((currentState,action,reward,nextState,terminalState))\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# train network\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainNetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;66;03m# set the current state for the next step\u001B[39;00m\n\u001B[0;32m    203\u001B[0m currentState\u001B[38;5;241m=\u001B[39mnextState\n",
      "Cell \u001B[1;32mIn[4], line 320\u001B[0m, in \u001B[0;36mDeepQLearning.trainNetwork\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    317\u001B[0m     outputNetwork[index,action]\u001B[38;5;241m=\u001B[39my\n\u001B[0;32m    319\u001B[0m \u001B[38;5;66;03m# here, we train the network\u001B[39;00m\n\u001B[1;32m--> 320\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmainNetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputNetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43moutputNetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatchReplayBufferSize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m     \n\u001B[0;32m    322\u001B[0m \u001B[38;5;66;03m# after updateTargetNetworkPeriod training sessions, update the coefficients \u001B[39;00m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;66;03m# of the target network\u001B[39;00m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;66;03m# increase the counter for training the target network\u001B[39;00m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcounterUpdateTargetNetwork\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m  \n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\engine\\training.py:1549\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1547\u001B[0m logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1548\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch, iterator \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39menumerate_epochs():\n\u001B[1;32m-> 1549\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1550\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_epoch_begin(epoch)\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\engine\\training.py:2319\u001B[0m, in \u001B[0;36mModel.reset_metrics\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2300\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resets the state of all the metrics in the model.\u001B[39;00m\n\u001B[0;32m   2301\u001B[0m \n\u001B[0;32m   2302\u001B[0m \u001B[38;5;124;03mExamples:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2316\u001B[0m \n\u001B[0;32m   2317\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2318\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics:\n\u001B[1;32m-> 2319\u001B[0m     \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\metrics\\base_metric.py:268\u001B[0m, in \u001B[0;36mMetric.reset_state\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_states()\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 268\u001B[0m     \u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_set_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvariables\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1174\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[0;32m   1175\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1176\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_target(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m   1178\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[0;32m   1179\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[0;32m   1180\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\backend.py:4302\u001B[0m, in \u001B[0;36mbatch_set_value\u001B[1;34m(tuples)\u001B[0m\n\u001B[0;32m   4300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mor\u001B[39;00m tf\u001B[38;5;241m.\u001B[39minside_function():\n\u001B[0;32m   4301\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x, value \u001B[38;5;129;01min\u001B[39;00m tuples:\n\u001B[1;32m-> 4302\u001B[0m         \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4303\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   4304\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_graph()\u001B[38;5;241m.\u001B[39mas_default():\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:941\u001B[0m, in \u001B[0;36mBaseResourceVariable.assign\u001B[1;34m(self, value, use_locking, name, read_value)\u001B[0m\n\u001B[0;32m    939\u001B[0m   validate_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_shape \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape\u001B[38;5;241m.\u001B[39mis_fully_defined()\n\u001B[0;32m    940\u001B[0m   kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidate_shape\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m validate_shape\n\u001B[1;32m--> 941\u001B[0m assign_op \u001B[38;5;241m=\u001B[39m gen_resource_variable_ops\u001B[38;5;241m.\u001B[39massign_variable_op(\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle, value_tensor, name\u001B[38;5;241m=\u001B[39mname, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    943\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m read_value:\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lazy_read(assign_op)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:141\u001B[0m, in \u001B[0;36massign_variable_op\u001B[1;34m(resource, value, validate_shape, name)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[0;32m    140\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 141\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAssignVariableOp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalidate_shape\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m      \u001B[49m\u001B[43mvalidate_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m    145\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "# instead of gym, import gymnasium \n",
    "#import gymnasium as gym\n",
    "\n",
    "# create environment\n",
    "env=gym.make('CartPole-v1')\n",
    "\n",
    "# select the parameters\n",
    "gamma=1\n",
    "# probability parameter for the epsilon-greedy approach\n",
    "epsilon=0.1\n",
    "# number of training episodes\n",
    "# NOTE HERE THAT AFTER CERTAIN NUMBERS OF EPISODES, WHEN THE PARAMTERS ARE LEARNED\n",
    "# THE EPISODE WILL BE LONG, AT THAT POINT YOU CAN STOP THE TRAINING PROCESS BY PRESSING CTRL+C\n",
    "# DO NOT WORRY, THE PARAMETERS WILL BE MEMORIZED\n",
    "numberEpisodes=1000\n",
    "\n",
    "# create an object\n",
    "LearningQDeep=DeepQLearning(env,gamma,epsilon,numberEpisodes)\n",
    "# run the learning process\n",
    "LearningQDeep.trainingEpisodes()\n",
    "# get the obtained rewards in every episode\n",
    "LearningQDeep.sumRewardsEpisode\n",
    "\n",
    "#  summarize the model\n",
    "LearningQDeep.mainNetwork.summary()\n",
    "# save the model, this is important, since it takes long time to train the model \n",
    "# and we will need model in another file to visualize the trained model performance\n",
    "LearningQDeep.mainNetwork.save(\"trained_model_temp.h5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T09:47:07.270319100Z",
     "start_time": "2023-08-30T09:46:41.806677500Z"
    }
   },
   "id": "546aa1d1906a6d35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d13a97fb77f4f4c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
