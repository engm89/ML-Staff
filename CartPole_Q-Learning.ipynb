{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T10:38:41.852233400Z",
     "start_time": "2023-08-29T10:38:37.694791300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44e3ec6e3e85a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T11:01:01.175785700Z",
     "start_time": "2023-08-29T11:01:01.159460300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Q_learning:\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, epsilon, numberEpisodes, numberBins, lowerBounds, upperBound):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.numberEpisodes = numberEpisodes\n",
    "        self.numberBins = numberBins\n",
    "        self.lowerBounds = lowerBounds\n",
    "        self.upperBound = upperBound\n",
    "        self.actionNumber = env.action_space.n\n",
    "        self.sumofRewards = []\n",
    "               self.Qmatrix=np.random.uniform(low=0, high=1, size=(numberOfBins[0],numberOfBins[1],numberOfBins[2],numberOfBins[3],self.actionNumber))\n",
    "    def returnIndexState(self, state):\n",
    "        position = state[0]\n",
    "        velocity = state[1]\n",
    "        angle = state[2]\n",
    "        angularVelocity = state[3]\n",
    "\n",
    "        cartPositinBin = np.linspace(self.lowerBounds[0], self.upperBound[0], self.numberBins[0])\n",
    "\n",
    "        velocityBin = np.linspace(self.lowerBounds[1], self.upperBound[1], self.numberBins[1])\n",
    "\n",
    "        angleBin = np.linspace(self.lowerBounds[2], self.upperBound[2], self.numberBins[2])\n",
    "\n",
    "        angularVelocityBin = np.linspace(self.lowerBounds[3], self.upperBound[3], self.numberBins[3])\n",
    "        \n",
    "        indexPostion = np.maximum(np.digitize(state[0], cartPositinBin)-1,0)\n",
    "        indexVelocity = np.maximum(np.digitize(state[1], velocityBin)-1,0)\n",
    "        indexAngle = np.maximum(np.digitize(state[2], angleBin)-1,0)\n",
    "        indexAngularVelocity = np.maximum(np.digitize(state[3], angularVelocityBin)-1,0)\n",
    "        \n",
    "        return tuple([indexPostion,indexVelocity, indexAngle, indexAngularVelocity])\n",
    "        \n",
    "    def selectAction(self, state, index):\n",
    "        \n",
    "        if index < 500:\n",
    "            return np.random.choice(self.actionNumber)\n",
    "        randomNumber = np.random.random()\n",
    "        \n",
    "        if index > 7000:\n",
    "            self.epsilon=0.9*self.epsilon\n",
    "        \n",
    "        if randomNumber < self.epsilon:\n",
    "            return np.random.choice(self.actionNumber)\n",
    "        else:\n",
    "            return np.random.choice(np.where(self.Qmatrix[self.returnIndexState(state)] == np.max(self.Qmatrix[self.returnIndexState(state)]))[0])\n",
    "        \n",
    "\n",
    "    def simulateEpisodes(self):\n",
    "        \n",
    "        for indexEpisode in range(self.numberEpisodes):\n",
    "            rewardsEpisode = []\n",
    "            (stateS,_)= self.env.reset()\n",
    "            stateS = list(stateS)\n",
    "            print('Episode {}'.format(indexEpisode))\n",
    "            terminalState = False\n",
    "            while not terminalState:\n",
    "                stateSIndex = self.returnIndexState(stateS)\n",
    "                actionA = self.selectAction(stateS, indexEpisode)\n",
    "                (stateSprime, reward, terminalState,_,_)= self.env.step(actionA)\n",
    "                rewardsEpisode.append(reward)\n",
    "                stateSprime= list(stateSprime)\n",
    "                stateSprimeIndex = self.returnIndexState(stateSprime)\n",
    "                QmaxPrime = np.max(self.Qmatrix[stateSprimeIndex])                \n",
    "                if not terminalState:\n",
    "                    error = reward+self.gamma*QmaxPrime-self.Qmatrix[stateSIndex+(actionA,)]\n",
    "                    self.Qmatrix[stateSIndex+(actionA,)] = self.Qmatrix[stateSIndex+(actionA,)]+self.alpha*error\n",
    "                else:\n",
    "                    error = reward-self.Qmatrix[stateSIndex+(actionA,)]\n",
    "                    self.Qmatrix[stateSIndex+(actionA,)] = self.Qmatrix[stateSIndex+(actionA,)] + self.alpha*error\n",
    "                stateS = stateSprime\n",
    "            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))\n",
    "            self.sumofRewards.append(np.sum(rewardsEpisode))\n",
    "\n",
    "    def simulateLearnedStrategy(self):\n",
    "        import gym \n",
    "        import time\n",
    "        env1=gym.make('CartPole-v1',render_mode='human')\n",
    "        (currentState,_)=env1.reset()\n",
    "        env1.render()\n",
    "        timeSteps=1000\n",
    "        # obtained rewards at every time step\n",
    "        obtainedRewards=[]\n",
    "        \n",
    "        for timeIndex in range(timeSteps):\n",
    "            print(timeIndex)\n",
    "            # select greedy actions\n",
    "            actionInStateS=np.random.choice(np.where(self.Qmatrix[self.returnIndexState(currentState)]==np.max(self.Qmatrix[self.returnIndexState(currentState)]))[0])\n",
    "            currentState, reward, terminated, truncated, info =env1.step(actionInStateS)\n",
    "            obtainedRewards.append(reward)   \n",
    "            time.sleep(0.05)\n",
    "            if (terminated):\n",
    "                time.sleep(1)\n",
    "                break\n",
    "        return obtainedRewards,env1\n",
    "\n",
    "    def simulateRandomStrategy(self):\n",
    "        import gym \n",
    "        import time\n",
    "        import numpy as np\n",
    "        env2=gym.make('CartPole-v1')\n",
    "        (currentState,_)=env2.reset()\n",
    "        env2.render()\n",
    "        # number of simulation episodes\n",
    "        episodeNumber=100\n",
    "        # time steps in every episode\n",
    "        timeSteps=1000\n",
    "        # sum of rewards in each episode\n",
    "        sumRewardsEpisodes=[]\n",
    "        \n",
    "        \n",
    "        for episodeIndex in range(episodeNumber):\n",
    "            rewardsSingleEpisode=[]\n",
    "            initial_state=env2.reset()\n",
    "            print(episodeIndex)\n",
    "            for timeIndex in range(timeSteps):\n",
    "                random_action=env2.action_space.sample()\n",
    "                observation, reward, terminated, truncated, info =env2.step(random_action)\n",
    "                rewardsSingleEpisode.append(reward)\n",
    "                if (terminated):\n",
    "                    break      \n",
    "            sumRewardsEpisodes.append(np.sum(rewardsSingleEpisode))\n",
    "        return sumRewardsEpisodes,env2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc4add9962942a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-29T11:01:02.678406400Z",
     "start_time": "2023-08-29T11:01:02.637983200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m Q1\u001B[39m=\u001B[39mQ_learning(env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds)\n\u001B[0;32m     32\u001B[0m \u001B[39m# run the Q-Learning algorithm\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m Q1\u001B[39m.\u001B[39;49msimulateEpisodes()\n\u001B[0;32m     34\u001B[0m \u001B[39m# simulate the learned strategy\u001B[39;00m\n\u001B[0;32m     35\u001B[0m (obtainedRewardsOptimal,env1)\u001B[39m=\u001B[39mQ1\u001B[39m.\u001B[39msimulateLearnedStrategy()\n",
      "Cell \u001B[1;32mIn[4], line 67\u001B[0m, in \u001B[0;36mQ_learning.simulateEpisodes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     65\u001B[0m stateSprime\u001B[39m=\u001B[39m \u001B[39mlist\u001B[39m(stateSprime)\n\u001B[0;32m     66\u001B[0m stateSprimeIndex \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mreturnIndexState(stateSprime)\n\u001B[1;32m---> 67\u001B[0m QmaxPrime \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39mmax(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mQmatrix[stateSprimeIndex])                \n\u001B[0;32m     68\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m terminalState:\n\u001B[0;32m     69\u001B[0m     error \u001B[39m=\u001B[39m reward\u001B[39m+\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mgamma\u001B[39m*\u001B[39mQmaxPrime\u001B[39m-\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mQmatrix[stateSIndex\u001B[39m+\u001B[39m(actionA,)]\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for array: array is 2-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "(state,_)=env.reset()\n",
    "#env.render()\n",
    "#env.close()\n",
    " \n",
    "# here define the parameters for state discretization\n",
    "upperBounds=env.observation_space.high\n",
    "lowerBounds=env.observation_space.low\n",
    "cartVelocityMin=-3\n",
    "cartVelocityMax=3\n",
    "poleAngleVelocityMin=-10\n",
    "poleAngleVelocityMax=10\n",
    "upperBounds[1]=cartVelocityMax\n",
    "upperBounds[3]=poleAngleVelocityMax\n",
    "lowerBounds[1]=cartVelocityMin\n",
    "lowerBounds[3]=poleAngleVelocityMin\n",
    " \n",
    "numberOfBinsPosition=30\n",
    "numberOfBinsVelocity=30\n",
    "numberOfBinsAngle=30\n",
    "numberOfBinsAngleVelocity=30\n",
    "numberOfBins=[numberOfBinsPosition,numberOfBinsVelocity,numberOfBinsAngle,numberOfBinsAngleVelocity]\n",
    " \n",
    "# define the parameters\n",
    "alpha=0.1\n",
    "gamma=1\n",
    "epsilon=0.2\n",
    "numberEpisodes=15000\n",
    " \n",
    "# create an object\n",
    "Q1=Q_learning(env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds)\n",
    "# run the Q-Learning algorithm\n",
    "Q1.simulateEpisodes()\n",
    "# simulate the learned strategy\n",
    "(obtainedRewardsOptimal,env1)=Q1.simulateLearnedStrategy()\n",
    " \n",
    "plt.figure(figsize=(12, 5))\n",
    "# plot the figure and adjust the plot parameters\n",
    "plt.plot(Q1.sumRewardsEpisode,color='blue',linewidth=1)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.savefig('convergence.png')\n",
    " \n",
    " \n",
    "# close the environment\n",
    "env1.close()\n",
    "# get the sum of rewards\n",
    "np.sum(obtainedRewardsOptimal)\n",
    " \n",
    "# now simulate a random strategy\n",
    "(obtainedRewardsRandom,env2)=Q1.simulateRandomStrategy()\n",
    "plt.hist(obtainedRewardsRandom)\n",
    "plt.xlabel('Sum of rewards')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('histogram.png')\n",
    "plt.show()\n",
    " \n",
    "# run this several times and compare with a random learning strategy\n",
    "(obtainedRewardsOptimal,env1)=Q1.simulateLearnedStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da25532c6fa1bdb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
