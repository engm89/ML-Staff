{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPSILON =1\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "GAMMA = 0.95\n",
    "LAMBDA = 0.0005\n",
    "TAU = 0.08\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "REWARD_STD = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmnet = gym.make(\"CartPole-v1\")\n",
    "NUM_STATE=4\n",
    "NUM_ACTIONS= environmnet.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpirienceRelay:\n",
    "    def __init__(self, maxlen=2000):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "    def get_batch(self, batch_size):\n",
    "        if no_sample > len(self._sample):\n",
    "            return random.sample(self.buffer, len(self._sample))\n",
    "        else:\n",
    "            return random.sample(self.buffer, batch_size)\n",
    "    def get_array_from_batch(self, batch):\n",
    "        state = np.array(x[0] for x in batch)\n",
    "        action = np.array(x[1] for x in batch)\n",
    "        rewards = np.array(x[2] for x in batch)\n",
    "        next_state = np.array([np.zeros(NUM_STATE) if x[3] in None else x[3]] for x in batch)\n",
    "\n",
    "        return state, action, rewards, next_state\n",
    "    @property\n",
    "    def buffer_size(self):\n",
    "        return len(self._buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, expirience_reply, state_size, actions_size, optimizer):\n",
    "\n",
    "        self.expirience_reply= expirience_reply\n",
    "        self.state_size = state_size\n",
    "        self.actions_size = actions_size\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.epsilon = MAX_EPSILON\n",
    "\n",
    "        self.primary_network = self.build_networks()\n",
    "        self.primary_network.compile(loss='mse', optimizer=self.optimizer)\n",
    "        self.target_network = self.build_network()\n",
    "    def build_networks (self):\n",
    "        network = Sequential()\n",
    "        network.add(Dense(30, activation='relu', kernel_initializer=he_normal()))\n",
    "        network.add(Dense(30, activation='relu', kernel_initializer=he_normal()))\n",
    "        network.add(Dense(self.actions_size))\n",
    "        return network\n",
    "    def act(self, state):\n",
    "        if np.random.rand < self.epsilon:\n",
    "            return np.random.randint(0, self.actions_size-1)\n",
    "        else:\n",
    "            q_values = self.primary_network(state.reshape(1, -1))\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def align_epsilon(self, step):\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * step)\n",
    "\n",
    "    def align_target_network(self):\n",
    "        for t, e in zip(self.target_network.trainable_variables, \\\n",
    "                        self.primary_network.trainable_variables): t.assign(t * (1 - TAU) + e * TAU)\n",
    "\n",
    "\n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.expirience_reply.store(state, action, reward, next_state, terminated)\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if self.expirience_reply.buffer_size < BATCH_SIZE *3:\n",
    "            return 0\n",
    "\n",
    "        batch = self.expirience_reply.get_batch(batch_size)\n",
    "        states, actions, rewards, next_states = self.expirience_replay.get_arrays_from_batch(batch)\n",
    "\n",
    "        q_values_state = self.primary_network(states).numpy()\n",
    "        q_values_next_state = self.primary_network(next_states).numpy()\n",
    "\n",
    "        target = q_values_state\n",
    "        updates = np.zeros(rewards.shape)\n",
    "\n",
    "        valid_indexes = np.array(next_states).sum(axis=1) !=0\n",
    "        batch_indexes = np.arange(BATCH_SIZE)\n",
    "\n",
    "        action = np.argmax(q_values_next_state, axis=1)\n",
    "        q_next_state_target = self.target_network(next_states)\n",
    "        updates[valid_indexes] = rewards[valid_indexes] + GAMMA *\\\n",
    "            q_next_state_target.numpy()[batch_indexes[valid_indexes], action[valid_indexes]]\n",
    "        \n",
    "        target[batch_indexes, actions] = updates\n",
    "        loss = self.primary_network.train_on_batch(states, target)\n",
    "\n",
    "        self.align_target_network()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTrainer():\n",
    "    def __init__(self, agent, environmnet):\n",
    "        self.agent = agent\n",
    "        self.environmnet=environmnet\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        next_state, reward, terminated,_= self.environmnet.step(action)\n",
    "        next_state = next_state if not terminated else None\n",
    "        reward = np.random.normal(1.0, REWARD_STD)\n",
    "        return next_state, reward, terminated\n",
    "    def _print_epoch_values(self, episode, total_epoch_reward, average_loss):\n",
    "        print(\"**********************************\")\n",
    "        print(f\"Episode: {episode} - Reward: {total_epoch_reward} - Average Loss: {average_loss:.3f}\")\n",
    "\n",
    "    def train(self, num_of_episodes=1000):\n",
    "        total_timesteps= 0\n",
    "        for episode in range(0, num_of_episodes):\n",
    "\n",
    "            state = self.environmnet.reset()\n",
    "\n",
    "            average_loss_per_episode = []\n",
    "            average_loss = 0\n",
    "            total_epoch_reward = 0\n",
    "\n",
    "            terminated = False\n",
    "\n",
    "            while not terminated:\n",
    "\n",
    "                action = self.agent.act(state)\n",
    "                next_state, reward, terminated = self._take_action(action)\n",
    "                self.agent.store(state, action, reward, next_state, terminated)\n",
    "                \n",
    "                loss = agent.train(BATCH_SIZE)\n",
    "                average_loss += loss\n",
    "\n",
    "                state = next_state\n",
    "                self.agent.align_epsilon(total_timesteps)\n",
    "                total_timesteps += 1\n",
    "\n",
    "                if terminated:\n",
    "                    average_loss /= total_epoch_reward\n",
    "                    average_loss_per_episode.append(average_loss)\n",
    "                    self._print_epoch_values(episode, total_epoch_reward, average_loss)\n",
    "                \n",
    "                # Real Reward is always 1 for Cart-Pole enviroment\n",
    "                total_epoch_reward +=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "expirience_replay = EpirienceRelay(50000)\n",
    "agent = DDQNAgent(expirience_replay, NUM_STATE, NUM_ACTIONS, optimizer)\n",
    "agent_trainer = AgentTrainer(agent, environmnet)\n",
    "agent_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
